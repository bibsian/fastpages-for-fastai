{
  
    
        "post0": {
            "title": "Action Detection - Part 1",
            "content": "import pprint as pp import cv2 import pandas as pd from fastai2.basics import * from fastai2.vision.all import * . The Goals . This is the first in a series of post where I start exploring how to build a real-time video classificaiton system. Warning, I&#39;m not a seasoned deep learning practioner and I&#39;ve never worked with video data so if you&#39;re very experience in this area you might want to stop reading here. If you&#39;re like me and new to this space then welcome, hopefully you&#39;ll find something that helps you on your journey. . In the rest of this post I&#39;m going do the following: . Outline the data I&#39;ll be working with | Create some utility functions to work with videos | Explore basic data augmentation | Explore creating Dataset objects | . and I&#39;ll also put forth some ideas and thoughts that have come to mind while getting my hands dirty. Lastly, I&#39;ll be sprinkling useful links throughout the post. . Setup &amp; Configuration with Some Gotchya&#39;s . First you&#39;re going to have to have some libraries installed for working with video. . My first set of googling led me to install openCV through conda-forge: . conda install -c conda-forge opencv . After starting down this path though, I came across some information that video&#39;s will now be first class citizens in PyTorch (version 0.4.0). You can read about the release here. The TLDR of the release notes are that they&#39;re building in video transforms, IO support through PyAV and adding to their model zoo for transfer learning. . That said, I ended up installing both libaries and I&#39;ll be using both; starting out with openCV and moving over to PyAV. One major difference you should consider before chosing a library to work with is that if you ever plan to use your code on an edge device you might be stuck with one or the other. I believe the Jetson Nano only supports gstreamer as a backend with support for openCV but PyAV utilizes ffmpeg so there may be issues trying to embed PyAV code on the Jeston Nano... maybe, this isn&#39;t confirmed but could be an issue and worth researching. . The Data . I chose to work with a small subset of classes from UCF-101&#39;s dataset: . Download the full dataset here | Check out the original paper describing the data here | . In brief, UCF-101 contains 101 videos of various types of actions; human-object interactions, human body motions, human-human interactions, playing instruments, and sports. . The subset of videos I&#39;m working with are all sports related. I downloaded data and reorganized the subset in a directory I&#39;m calling DATAPATH: . DATAPATH = Path(&quot;/home/bibsian/Desktop/action_subset&quot;) . pp.pprint([x for x in DATAPATH.ls()]) . [Path(&#39;/home/bibsian/Desktop/action_subset/BalanceBeam&#39;), Path(&#39;/home/bibsian/Desktop/action_subset/trainlist_subset.txt~&#39;), Path(&#39;/home/bibsian/Desktop/action_subset/trainlist_subset.txt&#39;), Path(&#39;/home/bibsian/Desktop/action_subset/CliffDiving&#39;), Path(&#39;/home/bibsian/Desktop/action_subset/testlist_subset.txt&#39;), Path(&#39;/home/bibsian/Desktop/action_subset/BasketballDunk&#39;)] . Here are the class labels I&#39;m working with (the above output is a little hard to read): . video_subsets = [x.name for x in sorted(Path(DATAPATH).ls()) if &quot;txt&quot; not in x.name] print(video_subsets) . [&#39;BalanceBeam&#39;, &#39;BasketballDunk&#39;, &#39;CliffDiving&#39;] . And just an FYI, the clip duration for these videos are somewhere between 3-6 seconds. And here&#39;s the label for an actual video: . (DATAPATH/&quot;BalanceBeam&quot;).ls()[0].name . &#39;v_BalanceBeam_g25_c04.avi&#39; . Train/Validation Labels . As we can see from above, each video label has the following syntax: v_[Action Label]_g[Group Number]_c[Clip Number] . When creating the train and validation sets you need to make sure that no clips from the same group are represented in both of the sets, otherwise there would be some data leakage and you&#39;d get better than usual performance. . The source was kind enough to provide text files for multiple train/validaiton sets but since I&#39;m working with a subset I just handmade the splits to account for that detail above; let&#39;s read the labels in and check them out. . df_train = pd.read_csv(Path(DATAPATH)/&quot;trainlist_subset.txt&quot;, header=None, names=[&quot;label&quot;]) df_valid = pd.read_csv(Path(DATAPATH)/&quot;testlist_subset.txt&quot;, header=None, names=[&quot;label&quot;]) . df_train.label.head(4) . 0 BalanceBeam/v_BalanceBeam_g08_c01.avi 1 BalanceBeam/v_BalanceBeam_g08_c02.avi 2 BalanceBeam/v_BalanceBeam_g08_c03.avi 3 BalanceBeam/v_BalanceBeam_g08_c04.avi Name: label, dtype: object . df_valid.label.head(4) . 0 BalanceBeam/v_BalanceBeam_g01_c01.avi 1 BalanceBeam/v_BalanceBeam_g01_c02.avi 2 BalanceBeam/v_BalanceBeam_g01_c03.avi 3 BalanceBeam/v_BalanceBeam_g01_c04.avi Name: label, dtype: object . Working with the data . So now that you know how to create the train/validation splits let&#39;s start writing some code to get a feel for working with videos. . Image Shapes . Before we start working with videos let take a step back and think about how we usually work with images. . You can typically represent a regular colored image in the form of a tensor with dimensions, $(C, H, W)$, where $C=Channel$, $H=height$, and $W=width$. Then when passing the data through a network you can tack on the batch size to the front of that tensor so images end up getting processed on the GPU in batches via $(N, C, H, W)$ dimensions, where $N$ is the batch size. . That said though, when we start working with videos we need to think about adding a dimension for time. And when you do this, tensors become dimensions of $(N, C, T, H, W)$ where all previous dimensions stay the same but you add $T=time$ to account for a stack of images that are essentially the clips of the video (a single clip is a frame i.e. a single image). And when you actually get around to training a model you can pass these dimension into a nn.Conv3d convolution. . But before we start playing with ath let&#39;s just get a feel for video tensors. . Video Tensors . Paths for sample videos . Here I&#39;m setting the Path for a few sample videos . def take_file_from_path(path, list_ix=0): &quot;&quot;&quot; Helper function to take first file from a path&quot;&quot;&quot; return str((path).ls()[list_ix]) BALANCE_TEST, DUNK_TEST, CLIFF_TEST = [take_file_from_path(DATAPATH/x) for x in video_subsets] . print(BALANCE_TEST) . /home/bibsian/Desktop/action_subset/BalanceBeam/v_BalanceBeam_g25_c04.avi . Video utils . Here&#39;s some functions that will help us work with videos in a very basic way. . from functools import partial . def get_video_feature(video_cap, feature): return video_cap.get(feature) vid_feature = lambda feature: partial(get_video_feature, feature=feature) get_n_frames = vid_feature(cv2.CAP_PROP_FRAME_COUNT) get_width = vid_feature(cv2.CAP_PROP_FRAME_WIDTH) get_height = vid_feature(cv2.CAP_PROP_FRAME_HEIGHT) get_fps = vid_feature(cv2.CAP_PROP_FPS) # frames per second def get_shape(video_cap): return int(get_height(video_cap)), int(get_width(video_cap)) def get_frame_features(video_cap): return int(get_fps(video_cap)), int(get_n_frames(video_cap)) . def describe_video(video_cap: cv2.VideoCapture): w, h = get_shape(video_cap) fps, n_frames = get_frame_features(video_cap) print(f&quot;Height: {h}, Width: {w}, FPS: {fps}, n_frames: {n_frames}&quot;) . def standard_reader(stream_cap, n_frames=30): frames = [] # Will be (T, C, H, W) format for f_index in range(n_frames): _, image = stream_cap.read() frames.append(image) assert frames, f&quot;Check file path, no frames coming from stream capture&quot; return frames . def read_video_file(path, n_frames=30, silence=True): &quot;&quot;&quot; Read video from begining and return first n_frames &quot;&quot;&quot; stream_capture = cv2.VideoCapture(str(path)) if not silence: print(f&quot;Test {str(path)}&quot;); describe_video(stream_capture) return standard_reader(stream_capture, n_frames) . Now that we have some primatives to read in a video and return a PyTorch tensor we need to rearrange it so it&#39;s in the dimension we&#39;re looking for $(C, T, H, W)$. . # reorder video tensor shape to (C,T,H,W) video_to_tensor = lambda frames: tensor(frames).permute(3,0,1,2) . Okay, so let&#39;s read some videos and turn them into tensors and check out some data . Test reads . cliff_tensor = video_to_tensor(read_video_file(CLIFF_TEST, silence=False)) . Test /home/bibsian/Desktop/action_subset/CliffDiving/v_CliffDiving_g01_c06.avi Height: 320, Width: 240, FPS: 25, n_frames: 76 . cliff_tensor.shape . torch.Size([3, 30, 240, 320]) . We&#39;re going to use a fast.ai helper function here for plotting images (show_images) . show_images(cliff_tensor[2,1:6,...], imsize=5) . Cool. We were able to read in the data, reorder the indeces so they&#39;re in the $(C,T,H,W)$ shape and view a snippet of the frames. Let&#39;s check out a sample of the other video action subsets I&#39;m working with: . Obviously that one above is a divers, so let&#39;s check out basketball dunk. . show_images(video_to_tensor(read_video_file(DUNK_TEST, silence=False))[2, 1:6, ...], imsize=5) . Test /home/bibsian/Desktop/action_subset/BasketballDunk/v_BasketballDunk_g16_c01.avi Height: 320, Width: 240, FPS: 25, n_frames: 76 . And here we have Balance Beams . balance_tensor = video_to_tensor(read_video_file(BALANCE_TEST, silence=False)) . Test /home/bibsian/Desktop/action_subset/BalanceBeam/v_BalanceBeam_g25_c04.avi Height: 320, Width: 240, FPS: 25, n_frames: 102 . show_images(balance_tensor[2, 25:, ...], imsize=5) # index from 25 to end of shape (30 in this case) . Cool; we can read in video clips and turn them in to tensors. I want to reiterate the fact that I&#39;m still uncertain how to reshape a multi-channel 3D stack of images into a vector but it&#39;s something to think about. . Video Transforms &amp; Data Augmentation . Alright, now that we have a basic understanding of how to manipulate a video and cast it into a tensor I needed to figure out how we can go about transforming video tensor so that we can do data augmentation prior to training; think cropping, resizing, rotating, etc. . The thing about data augmentation in fast.ai is that I couldn&#39;t get them to work on video tensors; they only seem to work on a single pytorch image or PIL image. After poking around I found the pytorch video transforms code base (I couldn&#39;t get it to work with imports so I copied the source code and link below); check out all those transforms! . balance_tensor.shape . torch.Size([3, 30, 240, 320]) . # Torch reference # https://github.com/stephenyan1231/vision/blob/video_transforms/references/video_classification/transforms.py import torch import random def crop(vid, i, j, h, w): return vid[..., i:(i + h), j:(j + w)] def center_crop(vid, output_size): h, w = vid.shape[-2:] th, tw = output_size i = int(round((h - th) / 2.)) j = int(round((w - tw) / 2.)) return crop(vid, i, j, th, tw) def hflip(vid): return vid.flip(dims=(-1,)) # NOTE: for those functions, which generally expect mini-batches, we keep them # as non-minibatch so that they are applied as if they were 4d (thus image). # this way, we only apply the transformation in the spatial domain def resize(vid, size, interpolation=&#39;bilinear&#39;): # NOTE: using bilinear interpolation because we don&#39;t work on minibatches # at this level scale = None if isinstance(size, int): scale = float(size) / min(vid.shape[-2:]) size = None return torch.nn.functional.interpolate( vid, size=size, scale_factor=scale, mode=interpolation, align_corners=False) def pad(vid, padding, fill=0, padding_mode=&quot;constant&quot;): # NOTE: don&#39;t want to pad on temporal dimension, so let as non-batch # (4d) before padding. This works as expected return torch.nn.functional.pad(vid, padding, value=fill, mode=padding_mode) def to_normalized_float_tensor(vid): return vid.permute(3, 0, 1, 2).to(torch.float32) / 255 def normalize(vid, mean, std): shape = (-1,) + (1,) * (vid.dim() - 1) mean = torch.as_tensor(mean).reshape(shape) std = torch.as_tensor(std).reshape(shape) return (vid - mean) / std # Class interface class RandomCrop(object): def __init__(self, size): self.size = size @staticmethod def get_params(vid, output_size): &quot;&quot;&quot;Get parameters for ``crop`` for a random crop. &quot;&quot;&quot; h, w = vid.shape[-2:] th, tw = output_size if w == tw and h == th: return 0, 0, h, w i = random.randint(0, h - th) j = random.randint(0, w - tw) return i, j, th, tw def __call__(self, vid): i, j, h, w = self.get_params(vid, self.size) return crop(vid, i, j, h, w) class CenterCrop(object): def __init__(self, size): self.size = size def __call__(self, vid): return center_crop(vid, self.size) class Resize(object): def __init__(self, size): self.size = size def __call__(self, vid): return resize(vid, self.size) class ToFloatTensorInZeroOne(object): def __call__(self, vid): return to_normalized_float_tensor(vid) class Normalize(object): def __init__(self, mean, std): self.mean = mean self.std = std def __call__(self, vid): return normalize(vid, self.mean, self.std) class RandomHorizontalFlip(object): def __init__(self, p=0.5): self.p = p def __call__(self, vid): if random.random() &lt; self.p: return hflip(vid) return vid class Pad(object): def __init__(self, padding, fill=0): self.padding = padding self.fill = fill def __call__(self, vid): return pad(vid, self.padding, self.fill) . So let&#39;s see if these bad boys work: . balance_tensor.shape . torch.Size([3, 30, 240, 320]) . show_images(balance_tensor[1, 20:25, ...]) . cropped_balance_tensor = CenterCrop((128, 128))(balance_tensor.float()) . show_images(cropped_balance_tensor[1, 20:25, ...]) . Isn&#39;t that pretty cool! . Dataset objects . So, now we know how to split our data, read videos into memory and reshape as necessary, and augment the video tensors. The next logical step is to build some utilities for creating Dataset objects (tuples of (X,y) pairs where X is the training sample an y is the data label). . If you ran the code where we loaded the video with openCV you&#39;d realize it was pretttty slow. Doing this for a ton of videos and chunking them out into small clips (multiple clips per video) would take a while and also require us to build the Dataset classes ourselves. Luckily, pytorch has some utils for this that leverage PyAV library. . from torchvision.datasets.video_utils import VideoClips # Source: https://github.com/pytorch/vision/releases/tag/v0.4.0 class MyVideoDataset(object): def __init__(self, video_paths): self.video_clips = VideoClips(video_paths, clip_length_in_frames=16, frames_between_clips=1, frame_rate=15) def __getitem__(self, idx): video, audio, info, video_idx = self.video_clips.get_clip(idx) return video, audio def __len__(self): return self.video_clips.num_clips() . You might get an error when reading in data (see here); I edited the source code to fix it but there might be other work arounds if you run into the error linked. . So lets try and create a dataset from 2 videos . vids_for_ds = [str(x) for x in get_files((DATAPATH/&quot;BalanceBeam&quot;))][0:2] . len(vids_for_ds) # 2 videos . 2 . #hide_output ds = MyVideoDataset(vids_for_ds) . len(ds) . 106 . Let&#39;s look at a sample we created . ds[0][0].shape . torch.Size([16, 240, 320, 3]) . Looks like it took 16 frames but also didn&#39;t permute the dimension to what we want so lets use a helper function from before, rearrange and check out the sample. . show_images(video_to_tensor(ds[4][0])[1, 1:8, ...]) . Closing . So there we have it. Hopefully you have a better understanding about how to leverage videos in your applications. Feel free to comment and thanks for reading. .",
            "url": "https://bibsian.github.io/fastpages-for-fastai/jupyter/2020/04/23/Action-Detection-Part-1.html",
            "relUrl": "/jupyter/2020/04/23/Action-Detection-Part-1.html",
            "date": " • Apr 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Poe Recognition",
            "content": "from fastai2.vision.all import * . Notebook Overview . I&#39;m going to create an image recognition model that predicts if a photo contains my pet Poe or not. The impetus for this post stems from the following: . I&#39;ve been curious about how &#39;easy&#39; it is to train a CNN that can distinguish individuals; I know google does it with my phone but can I do it with fast.ai and a few hundred photos that aren&#39;t individual specific? . | I&#39;d like to get a better handle on structuring a computer vision project with my own data and the fast.ai library. . | . and finally . I want to create my first post on the fastpages section of my blog. :-) | . SOTA: Dog or Cat . fast.ai makes training state of the art (SOTA) models a breeze with their high level API; take the cat classifier below: . from fastai2.vision.all import * path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.169193 | 0.020039 | 0.006089 | 00:19 | . epoch train_loss valid_loss error_rate time . 0 | 0.051610 | 0.011121 | 0.004736 | 00:26 | . With the 8 lines of code above we&#39;re able to get a near perfect binary classifier for predicting whether an image is a cat or not. You can assess the accuracy with the error_rate and if you think that&#39;s amazing, let&#39;s take a peak at what the model got wrong. . As you can see from the confusion matrix above and a subset of the most incorrect images (ranked by the loss value), that&#39;s a prettttty good classifier we just made, making some reasonable mistakes for a computer... . Understanding the magic . You might think that the small code snippet above was pure black magic but what&#39;s really going on is a ton of abstraction has been hidden away in layers of code. Roughly speaking, the steps that have been abstracted are the following: . Downloading data from an external URL | Creating objects which split the data into train and validation sets (dls i.e. data loaders) These objects also handle batching data for stochastic gradient descent (SGD). | . | Providing a mechanism for labeling (that indiciates what the model should be fitting to) | Defining transformations to the individual data points (photos) before they get batched (item_tfms), specifically resizing them to be all the same size (so we can use them on the GPU) | Instantiating a PyTorch model with pretrained weights (resnet34) Removing the last few layer of resnet34 and attaching new layers of our own (with some random initialization) | . | Assigning a loss function (using the default here which is inferred from the dls objects) | Setting a metric for monitoring performance | Using SGD to minimize our loss function . Training for an epoch where we only updated the new layers we put on top resenet34 | Then we training for another epoch where all weights could be updated and | . | Finally, utility functions for inspecting our results . | . While abstraction is really necessary for creating good readable code, it doesn&#39;t necessarily help us when we start to try and apply these tools to our own projects. So here&#39;s where we&#39;re going to start digging into the code and expand our understanding of what&#39;s happening under the hood by leveraging our own dataset and inspecting things as we go. . The data: Path objects, labels, and directory structures . The first thing we need to do for our classifier is firgure out how to structure our directories, how to label it and then pass that information into our DataLoaders. . Ignoring the fact that untar_data does some downloading magic, let&#39;s checkout what it returns: . path = untar_data(URLs.PETS)/&#39;images&#39;;path . Path(&#39;/home/bibsian/.fastai/data/oxford-iiit-pet/images&#39;) . isinstance(path, Path) . True . So turns out that untar_data passes back a Path object which contains our labeled data for the cat classifier. From the code in is_cat it looks like all cat photos should have a capitalized name; let&#39;s test that: . # Let&#39;s inspect the repo ! ls $path | head -5 . Abyssinian_100.jpg Abyssinian_100.mat Abyssinian_101.jpg Abyssinian_101.mat Abyssinian_102.jpg ls: write error: Broken pipe . Looks like we can grab the first image and see if it&#39;s a cat (here&#39;s some bash-python for you :-o) . cat_file = ! ls $path | head -1 . def show_img(path): return PILImage.create(path).show() . # The beauty of Path objects is you can just concatente inline with backslashes # (not sure if this works on windows though) show_img(path/cat_file[0]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9f08b52790&gt; . So this looks like a cat... cool! For good measure let&#39;s look at a dog. . ! ls $path | tail -5 # dogs dont . yorkshire_terrier_96.jpg yorkshire_terrier_97.jpg yorkshire_terrier_98.jpg yorkshire_terrier_99.jpg yorkshire_terrier_9.jpg . dog_file = ! ls $path | tail -1 show_img(path/dog_file[0]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9f17eed850&gt; . And there we go; we&#39;ve validated the labeling methodology. . So looks like we can throw all of our photos in 1 directory, label how ever we want to distinguish the classes and make sure we pass a function to our ImageDataLoaders that knowns how to parse it; pretty neat. . My data . Okay so here&#39;s where I go on a tanget about my data as well as providing the code to organize it. . The source: I downloaded all my google photos that were tagged with my dog, Poe. I manually inspected each of the photos (a little more than 1000) and made sure you could see his face in the photo; let&#39;s check one out... note, not all shots are nearly as good of a close up (a lot are from far away with other things in the frame): . poe_path = Path(&quot;/home/bibsian/Downloads/Poe&quot;) . poe_img = ! ls ~/Downloads/Poe | head -20 . show_img(poe_path/poe_img[4]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9ef573d5d0&gt; . And there&#39;s his big ole block head :-). If you&#39;re guessing what kind of dog he is my best guess is that he&#39;s a springer spaniel pitbull mix; google &quot;brown spinger spaniel&quot; if you want to get dogs that looks like him. I&#39;ll use those search terms to source negative sample images to use during training via Bing&#39;s Image Search API. . Since my Poe folder has around 1000 images and is about 3GB worth of data I&#39;m going to start training a model with a random subset. Here&#39;s what the next bit of code will do: . Create a folder called poe_data | Transfer a random subset of images and rename them with poe_&lt;numerical_index&gt; | . and . Download images of dogs that could look like him (i.e. use Bing API with &quot;brown springer spaniels&quot;), put them in poe_data, and label them Spaniel_&lt;numerical_index&gt; | . import numpy as np from typing import * . N_SUBSET = 150 poe_classifier_path = Path(&quot;/home/bibsian/Desktop/poe_data&quot;) . n_poe_photos = len(poe_path.ls()); print(n_poe_photos) . 989 . poe_unlabeled_files = poe_path.ls()[np.random.randint(0, n_poe_photos, N_SUBSET)] . if not poe_classifier_path.exists(): print(&quot;Creating source directory for Poe Classifier&quot;) poe_classifier_path.mkdir() . def move_files_and_rename_with_base_index(source_files: List[Path], dest_dir: Path, base_name: str): &quot;&quot;&quot; Move files and rename with a base string appended with an index i.e. &lt;basestr&gt;_0, &lt;basestr&gt;_1 &quot;&quot;&quot; for ix, og_file in enumerate(source_files): with (dest_dir/f&quot;{base_name}_{ix}{og_file.suffix}&quot;).open(mode=&quot;xb&quot;) as file_id: file_id.write(og_file.read_bytes()) . if not poe_classifier_path.ls(): print(&quot;Moving a subset of unlabeled Poe images&quot;) move_files_and_rename_with_base_index(source_files=poe_unlabeled_files, dest_dir=poe_classifier_path, base_name=&quot;poe&quot;) . Gathering negative samples for our classifier (i.e. photos not of Poe) . I&#39;m going to use Bing&#39;s image search API here but I&#39;m not including the code bits for it; the reason being is it&#39;s part of a prereleased library and it&#39;s against the terms of use, but you can easily query the API however you want, just know it returns some URL&#39;s of images that we have to download. . results = search_images_bing(BING_API, &#39;brown springer spaniel&#39;) ims = results.attrgot(&#39;content_url&#39;) . print(f&quot;Downloaded {len(ims)} brown spaniel images&quot;) . Downloaded 150 brown spaniel images . # Checking a download image spaniel_test_img = Path(&quot;/home/bibsian/Desktop/spaniel.jpg&quot;) if not spaniel_test_img.exists: download_url(ims[0], str(spaniel_test_img)) show_img(spaniel_test_img) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9ee849ec50&gt; . The downloaded image looks great; it&#39;s Poe like but definitely not my dog :-). . Now I&#39;m going to finish out the downloads and throw them all into the same directory as our labeled Poe images (poe_classifier_path). . spaniels_path = Path(&quot;/home/bibsian/Desktop/spaniels&quot;) if not spaniels_path.exists(): spaniels_path.mkdir() . if not spaniels_path.ls(): download_images(spaniels_path, urls=results.attrgot(&#39;content_url&#39;)) . spaniel_downloads = get_image_files(spaniels_path) . Let&#39;s check and see if any of the download images are corrupt: . failed = verify_images(spaniel_downloads) . # Looks good len(failed) . 0 . Okay now that we&#39;ve downloaded the files and verified they aren&#39;t corrupted, let&#39;s move them to the data directory we&#39;ll be using for the classifier and create our DataLoaders: . move_files_and_rename_with_base_index(source_files=spaniel_downloads, dest_dir=poe_classifier_path, base_name=&quot;spaniel&quot;) . Let&#39;s verify our poe_classifier_path has images of spaniels and Poe: . ! ls $poe_classifier_path | tail -3 . spaniel_98.jpg spaniel_99.jpg spaniel_9.jpg . ! ls $poe_classifier_path | head -3 . poe_0.jpg poe_100.jpg poe_101.jpg . Everything looks good from a directory structure standpoint now; onwards to the DataLoaders and model. . The DataLoaders object . Now that we organized our data let&#39;s instantiate the ImageDataLoaders object for our own use and define our labeling function: . def is_poe(x): &quot;&quot;&quot; x is the filename of a Path object&quot;&quot;&quot; return &quot;poe&quot; in x . poe_dls = ImageDataLoaders.from_name_func( path=poe_classifier_path, fnames=get_image_files(poe_classifier_path), label_func=is_poe, valid_pct=0.4, seed=10, item_tfms=Resize(124,124) ) . poe_dls.show_batch() . Now wasn&#39;t that easy :-) . Fine tuning our model . As a final step, we can take our DataLoaders object and fine tune it and see how well the model can distinguish Poe from other spaniels. . learner = cnn_learner(poe_dls, resnet34, metrics=error_rate) . learner.fine_tune(3) . epoch train_loss valid_loss error_rate time . 0 | 1.511148 | 1.483697 | 0.474576 | 00:15 | . epoch train_loss valid_loss error_rate time . 0 | 0.752346 | 0.442981 | 0.220339 | 00:15 | . 1 | 0.567987 | 0.156687 | 0.050847 | 00:14 | . 2 | 0.427543 | 0.141456 | 0.042373 | 00:14 | . That&#39;s a pretty decent model... The training loss keeps dropping, the validation loss keeps dropping, the training loss is less than validation loss with no signs of overfitting; not bad for a few lines of code. . Let&#39;s do a sense check of the model now and plot what it got wrong: . # learner.save(&quot;poe_classifier_fine_tuned&quot;) . poe_interpret = ClassificationInterpretation.from_learner(learner) . poe_interpret.plot_confusion_matrix() . poe_interpret.plot_top_losses(8) . Now what? . Looking at our top losses you might be a bit surprised to see how wrong it got pictures that were clearly my dog... but let&#39;s delve a bit deeper into this and check out some more images of Poe below: . poe_dls.show_batch() . You might not be able to tell from this subset but a lot of Poe&#39;s images have more than just him. So when I think about how the model could get shots of him so wrong it makes me think our model is learning about features that aren&#39;t specific to Poe but probably the background that makes up more of his photos :-/... . For now we&#39;re going to leave it here but next time we&#39;ll look into the what features of the image our model is really focusing on in order to distinguish Poe from other spaniels. . Thanks for reading and feel free to leave any comments, questions, or corrections! .",
            "url": "https://bibsian.github.io/fastpages-for-fastai/jupyter/2020/03/22/cnns.html",
            "relUrl": "/jupyter/2020/03/22/cnns.html",
            "date": " • Mar 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://bibsian.github.io/fastpages-for-fastai/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://bibsian.github.io/fastpages-for-fastai/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m Andrew Bibian and this is my repository of notebooks for learning about Deep Learning. This section of my blog is inspired by fast.ai; a group of amazing researchers whose mission is to democratize Deep Learning. They do this through teaching courses, putting out research, and developing high level libraries for practitioners. I feel lucky to have received a 2020 diversity fellowship for their USF’s Deep Learning Certificate Course. . I hope you find something here that expands your thinking. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://bibsian.github.io/fastpages-for-fastai/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}