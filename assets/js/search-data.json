{
  
    
        "post0": {
            "title": "Poe Recognition",
            "content": "from fastai2.vision.all import * . Notebook Overview . I&#39;m going to create an image recognition model that predicts if a photo contains my pet Poe or not. The impetus for this post stems from the following: . I&#39;ve been curious about how &#39;easy&#39; it is to train a CNN that can distinguish individuals; I know google does it with my phone but can I do it with fast.ai and a few hundred photos that aren&#39;t individual specific? . | I&#39;d like to get a better handle on structuring a computer vision project with my own data and the fast.ai library. . | . and finally . I want to create my first post on the fastpages section of my blog. :-) | . SOTA: Dog or Cat . fast.ai makes training state of the art (SOTA) models a breeze with their high level API; take the cat classifier below: . from fastai2.vision.all import * path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.169193 | 0.020039 | 0.006089 | 00:19 | . epoch train_loss valid_loss error_rate time . 0 | 0.051610 | 0.011121 | 0.004736 | 00:26 | . With the 8 lines of code above we&#39;re able to get a near perfect binary classifier for predicting whether an image is a cat or not. You can assess the accuracy with the error_rate and if you think that&#39;s amazing, let&#39;s take a peak at what the model got wrong. . As you can see from the confusion matrix above and a subset of the most incorrect images (ranked by the loss value), that&#39;s a prettttty good classifier we just made, making some reasonable mistakes for a computer... . Understanding the magic . You might think that the small code snippet above was pure black magic but what&#39;s really going on is a ton of abstraction has been hidden away in layers of code. Roughly speaking, the steps that have been abstracted are the following: . Downloading data from an external URL | Creating objects which split the data into train and validation sets (dls i.e. data loaders) These objects also handle batching data for stochastic gradient descent (SGD). | . | Providing a mechanism for labeling (that indiciates what the model should be fitting to) | Defining transformations to the individual data points (photos) before they get batched (item_tfms), specifically resizing them to be all the same size (so we can use them on the GPU) | Instantiating a PyTorch model with pretrained weights (resnet34) Removing the last few layer of resnet34 and attaching new layers of our own (with some random initialization) | . | Assigning a loss function (using the default here which is inferred from the dls objects) | Setting a metric for monitoring performance | Using SGD to minimize our loss function . Training for an epoch where we only updated the new layers we put on top resenet34 | Then we training for another epoch where all weights could be updated and | . | Finally, utility functions for inspecting our results . | . While abstraction is really necessary for creating good readable code, it doesn&#39;t necessarily help us when we start to try and apply these tools to our own projects. So here&#39;s where we&#39;re going to start digging into the code and expand our understanding of what&#39;s happening under the hood by leveraging our own dataset and inspecting things as we go. . The data: Path objects, labels, and directory structures . The first thing we need to do for our classifier is firgure out how to structure our directories, how to label it and then pass that information into our DataLoaders. . Ignoring the fact that untar_data does some downloading magic, let&#39;s checkout what it returns: . path = untar_data(URLs.PETS)/&#39;images&#39;;path . Path(&#39;/home/bibsian/.fastai/data/oxford-iiit-pet/images&#39;) . isinstance(path, Path) . True . So turns out that untar_data passes back a Path object which contains our labeled data for the cat classifier. From the code in is_cat it looks like all cat photos should have a capitalized name; let&#39;s test that: . # Let&#39;s inspect the repo ! ls $path | head -5 . Abyssinian_100.jpg Abyssinian_100.mat Abyssinian_101.jpg Abyssinian_101.mat Abyssinian_102.jpg ls: write error: Broken pipe . Looks like we can grab the first image and see if it&#39;s a cat (here&#39;s some bash-python for you :-o) . cat_file = ! ls $path | head -1 . def show_img(path): return PILImage.create(path).show() . # The beauty of Path objects is you can just concatente inline with backslashes # (not sure if this works on windows though) show_img(path/cat_file[0]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9f08b52790&gt; . So this looks like a cat... cool! For good measure let&#39;s look at a dog. . ! ls $path | tail -5 # dogs dont . yorkshire_terrier_96.jpg yorkshire_terrier_97.jpg yorkshire_terrier_98.jpg yorkshire_terrier_99.jpg yorkshire_terrier_9.jpg . dog_file = ! ls $path | tail -1 show_img(path/dog_file[0]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9f17eed850&gt; . And there we go; we&#39;ve validated the labeling methodology. . So looks like we can throw all of our photos in 1 directory, label how ever we want to distinguish the classes and make sure we pass a function to our ImageDataLoaders that knowns how to parse it; pretty neat. . My data . Okay so here&#39;s where I go on a tanget about my data as well as providing the code to organize it. . The source: I downloaded all my google photos that were tagged with my dog, Poe. I manually inspected each of the photos (a little more than 1000) and made sure you could see his face in the photo; let&#39;s check one out... note, not all shots are nearly as good of a close up (a lot are from far away with other things in the frame): . poe_path = Path(&quot;/home/bibsian/Downloads/Poe&quot;) . poe_img = ! ls ~/Downloads/Poe | head -20 . show_img(poe_path/poe_img[4]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9ef573d5d0&gt; . And there&#39;s his big ole block head :-). If you&#39;re guessing what kind of dog he is my best guess is that he&#39;s a springer spaniel pitbull mix; google &quot;brown spinger spaniel&quot; if you want to get dogs that looks like him. I&#39;ll use those search terms to source negative sample images to use during training via Bing&#39;s Image Search API. . Since my Poe folder has around 1000 images and is about 3GB worth of data I&#39;m going to start training a model with a random subset. Here&#39;s what the next bit of code will do: . Create a folder called poe_data | Transfer a random subset of images and rename them with poe_&lt;numerical_index&gt; | . and . Download images of dogs that could look like him (i.e. use Bing API with &quot;brown springer spaniels&quot;), put them in poe_data, and label them Spaniel_&lt;numerical_index&gt; | . import numpy as np from typing import * . N_SUBSET = 150 poe_classifier_path = Path(&quot;/home/bibsian/Desktop/poe_data&quot;) . n_poe_photos = len(poe_path.ls()); print(n_poe_photos) . 989 . poe_unlabeled_files = poe_path.ls()[np.random.randint(0, n_poe_photos, N_SUBSET)] . if not poe_classifier_path.exists(): print(&quot;Creating source directory for Poe Classifier&quot;) poe_classifier_path.mkdir() . def move_files_and_rename_with_base_index(source_files: List[Path], dest_dir: Path, base_name: str): &quot;&quot;&quot; Move files and rename with a base string appended with an index i.e. &lt;basestr&gt;_0, &lt;basestr&gt;_1 &quot;&quot;&quot; for ix, og_file in enumerate(source_files): with (dest_dir/f&quot;{base_name}_{ix}{og_file.suffix}&quot;).open(mode=&quot;xb&quot;) as file_id: file_id.write(og_file.read_bytes()) . if not poe_classifier_path.ls(): print(&quot;Moving a subset of unlabeled Poe images&quot;) move_files_and_rename_with_base_index(source_files=poe_unlabeled_files, dest_dir=poe_classifier_path, base_name=&quot;poe&quot;) . Gathering negative samples for our classifier (i.e. photos not of Poe) . I&#39;m going to use Bing&#39;s image search API here but I&#39;m not including the code bits for it; the reason being is it&#39;s part of a prereleased library and it&#39;s against the terms of use, but you can easily query the API however you want, just know it returns some URL&#39;s of images that we have to download. . results = search_images_bing(BING_API, &#39;brown springer spaniel&#39;) ims = results.attrgot(&#39;content_url&#39;) . print(f&quot;Downloaded {len(ims)} brown spaniel images&quot;) . Downloaded 150 brown spaniel images . # Checking a download image spaniel_test_img = Path(&quot;/home/bibsian/Desktop/spaniel.jpg&quot;) if not spaniel_test_img.exists: download_url(ims[0], str(spaniel_test_img)) show_img(spaniel_test_img) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9ee849ec50&gt; . The downloaded image looks great; it&#39;s Poe like but definitely not my dog :-). . Now I&#39;m going to finish out the downloads and throw them all into the same directory as our labeled Poe images (poe_classifier_path). . spaniels_path = Path(&quot;/home/bibsian/Desktop/spaniels&quot;) if not spaniels_path.exists(): spaniels_path.mkdir() . if not spaniels_path.ls(): download_images(spaniels_path, urls=results.attrgot(&#39;content_url&#39;)) . spaniel_downloads = get_image_files(spaniels_path) . Let&#39;s check and see if any of the download images are corrupt: . failed = verify_images(spaniel_downloads) . # Looks good len(failed) . 0 . Okay now that we&#39;ve downloaded the files and verified they aren&#39;t corrupted, let&#39;s move them to the data directory we&#39;ll be using for the classifier and create our DataLoaders: . move_files_and_rename_with_base_index(source_files=spaniel_downloads, dest_dir=poe_classifier_path, base_name=&quot;spaniel&quot;) . Let&#39;s verify our poe_classifier_path has images of spaniels and Poe: . ! ls $poe_classifier_path | tail -3 . spaniel_98.jpg spaniel_99.jpg spaniel_9.jpg . ! ls $poe_classifier_path | head -3 . poe_0.jpg poe_100.jpg poe_101.jpg . Everything looks good from a directory structure standpoint now; onwards to the DataLoaders and model. . The DataLoaders object . Now that we organized our data let&#39;s instantiate the ImageDataLoaders object for our own use and define our labeling function: . def is_poe(x): &quot;&quot;&quot; x is the filename of a Path object&quot;&quot;&quot; return &quot;poe&quot; in x . poe_dls = ImageDataLoaders.from_name_func( path=poe_classifier_path, fnames=get_image_files(poe_classifier_path), label_func=is_poe, valid_pct=0.4, seed=10, item_tfms=Resize(124,124) ) . poe_dls.show_batch() . Now wasn&#39;t that easy :-) . Fine tuning our model . As a final step, we can take our DataLoaders object and fine tune it and see how well the model can distinguish Poe from other spaniels. . learner = cnn_learner(poe_dls, resnet34, metrics=error_rate) . learner.fine_tune(3) . epoch train_loss valid_loss error_rate time . 0 | 1.511148 | 1.483697 | 0.474576 | 00:15 | . epoch train_loss valid_loss error_rate time . 0 | 0.752346 | 0.442981 | 0.220339 | 00:15 | . 1 | 0.567987 | 0.156687 | 0.050847 | 00:14 | . 2 | 0.427543 | 0.141456 | 0.042373 | 00:14 | . That&#39;s a pretty decent model... The training loss keeps dropping, the validation loss keeps dropping, the training loss is less than validation loss with no signs of overfitting; not bad for a few lines of code. . Let&#39;s do a sense check of the model now and plot what it got wrong: . # learner.save(&quot;poe_classifier_fine_tuned&quot;) . poe_interpret = ClassificationInterpretation.from_learner(learner) . poe_interpret.plot_confusion_matrix() . poe_interpret.plot_top_losses(8) . Now what? . Looking at our top losses you might be a bit surprised to see how wrong it got pictures that were clearly my dog... but let&#39;s delve a bit deeper into this and check out some more images of Poe below: . poe_dls.show_batch() . You might not be able to tell from this subset but a lot of Poe&#39;s images have more than just him. So when I think about how the model could get shots of him so wrong it makes me think our model is learning about features that aren&#39;t specific to Poe but probably the background that makes up more of his photos :-/... . For now we&#39;re going to leave it here but next time we&#39;ll look into the what features of the image our model is really focusing on in order to distinguish Poe from other spaniels. . Thanks for reading and feel free to leave any comments, questions, or corrections! .",
            "url": "https://bibsian.github.io/fastpages-for-fastai/jupyter/2020/03/22/cnns.html",
            "relUrl": "/jupyter/2020/03/22/cnns.html",
            "date": " • Mar 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://bibsian.github.io/fastpages-for-fastai/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://bibsian.github.io/fastpages-for-fastai/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m Andrew Bibian and this is my repository of notebooks for learning about Deep Learning. This section of my blog is inspired by fast.ai; a group of amazing researchers whose mission is to democratize Deep Learning. They do this through teaching courses, putting out research, and developing high level libraries for practitioners. I feel lucky to have received a 2020 diversity fellowship for their USF’s Deep Learning Certificate Course. . I hope you find something here that expands your thinking. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://bibsian.github.io/fastpages-for-fastai/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}